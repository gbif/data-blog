<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>GBIF and Apache-Spark on Microsoft Azure tutorial - GBIF Data Blog</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="John Waller" />
  <meta name="description" content="GBIF now has a snapshot of 1.3 billion occurrences✝ records on Microsoft Azure.
It is hosted by the Microsoft AI for Earth program, which hosts geospatial datasets that are important to environmental sustainability and Earth science. Hosting is convenient because you could now use occurrences in combination with other environmental layers and not need to upload any of it to the Azure. You can read previous discussions about GBIF and cloud computing here. The main reason you would want to use cloud computing is to run big data queries that are slow or impractical on a local machine.
" />

  <meta name="keywords" content="Hugo, theme, even" />






<meta name="generator" content="Hugo 0.91.2" />


<link rel="canonical" href="https://data-blog.gbif.org/post/microsoft-azure-and-gbif/" />

<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">







<link href="/dist/even.min.css?v=3.2.0" rel="stylesheet">




<link rel="stylesheet" href="/css//custom.css">


<meta property="og:title" content="GBIF and Apache-Spark on Microsoft Azure tutorial" />
<meta property="og:description" content="GBIF now has a snapshot of 1.3 billion occurrences✝ records on Microsoft Azure.
It is hosted by the Microsoft AI for Earth program, which hosts geospatial datasets that are important to environmental sustainability and Earth science. Hosting is convenient because you could now use occurrences in combination with other environmental layers and not need to upload any of it to the Azure. You can read previous discussions about GBIF and cloud computing here. The main reason you would want to use cloud computing is to run big data queries that are slow or impractical on a local machine." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://data-blog.gbif.org/post/microsoft-azure-and-gbif/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2021-05-19T00:00:00+00:00" />
<meta property="article:modified_time" content="2021-04-22T16:00:05+02:00" />

<meta itemprop="name" content="GBIF and Apache-Spark on Microsoft Azure tutorial">
<meta itemprop="description" content="GBIF now has a snapshot of 1.3 billion occurrences✝ records on Microsoft Azure.
It is hosted by the Microsoft AI for Earth program, which hosts geospatial datasets that are important to environmental sustainability and Earth science. Hosting is convenient because you could now use occurrences in combination with other environmental layers and not need to upload any of it to the Azure. You can read previous discussions about GBIF and cloud computing here. The main reason you would want to use cloud computing is to run big data queries that are slow or impractical on a local machine."><meta itemprop="datePublished" content="2021-05-19T00:00:00+00:00" />
<meta itemprop="dateModified" content="2021-04-22T16:00:05+02:00" />
<meta itemprop="wordCount" content="1175">
<meta itemprop="keywords" content="cloud computing,azure," /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="GBIF and Apache-Spark on Microsoft Azure tutorial"/>
<meta name="twitter:description" content="GBIF now has a snapshot of 1.3 billion occurrences✝ records on Microsoft Azure.
It is hosted by the Microsoft AI for Earth program, which hosts geospatial datasets that are important to environmental sustainability and Earth science. Hosting is convenient because you could now use occurrences in combination with other environmental layers and not need to upload any of it to the Azure. You can read previous discussions about GBIF and cloud computing here. The main reason you would want to use cloud computing is to run big data queries that are slow or impractical on a local machine."/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">GBIF Data Blog</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/categories/gbif/">
        <li class="mobile-menu-item">posts</li>
      </a><a href="https://discourse.gbif.org/c/data-blog">
        <li class="mobile-menu-item">community-forum</li>
      </a><a href="https://www.gbif.org/">
        <li class="mobile-menu-item">gbif.org</li>
      </a><a href="/about/">
        <li class="mobile-menu-item">about</li>
      </a>
  </ul>
</nav>
  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">
	<img src= "/logo.png" alt="GBIF-analytics-blog" style ="width:20%;">
  </a>
  
	
  
  
</div>

<nav class="site-navbar">

  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/categories/gbif/">posts</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="https://discourse.gbif.org/c/data-blog">community-forum</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="https://www.gbif.org/">gbif.org</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/about/">about</a>
      </li>
  </ul>
</nav>
    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
	
    <header class="post-header">
      <h1 class="post-title">GBIF and Apache-Spark on Microsoft Azure tutorial</h1>
	  
	  
      <div class="post-author">John Waller</div>

      <div class="post-meta">
        <span class="post-time"> 2021-05-19 </span>
        <div class="post-category">
            
              <a href="/categories/gbif/"> GBIF </a>
            
          </div>
        
        
      </div>
    </header>

    
    

    
    

    
    <div class="post-content">
      <p><strong>GBIF</strong> now has a <a href="https://github.com/microsoft/AIforEarthDataSets/blob/main/data/gbif.md">snapshot</a> of 1.3 billion occurrences<sub>✝</sub> records on <a href="https://www.microsoft.com/en-us/ai/ai-for-earth">Microsoft Azure</a>.</p>
<p>It is hosted by the <strong>Microsoft AI for Earth program</strong>, which hosts geospatial datasets that are important to environmental sustainability and Earth science. Hosting is convenient because you could now use occurrences in combination with other environmental layers and not need to upload any of it to the Azure. You can <strong>read previous discussions about GBIF and cloud computing</strong> <a href="https://discourse.gbif.org/t/gbif-exports-as-public-datasets-in-cloud-environments/1835">here</a>. The main reason you would want to use cloud computing is to run <strong>big data queries</strong> that are slow or impractical on a local machine.</p>
<p>In this tutorial, I will be running a simple query on 1.3 billion occurrences records. I will be using <a href="https://spark.apache.org/">apache-spark</a>. Spark has APIs in <strong>R</strong>, <strong>scala</strong>, and <strong>python</strong>.</p>
<p><sub>✝</sub>The snapshot includes all records shared under CC0 and CC BY designations published through GBIF that have coordinates which have passed automated quality checks. The GBIF mediated occurrence data are stored in Parquet files in Azure Blob Storage in the West Europe Azure region. The periodic occurrence snapshots are stored in occurrence/YYYY-MM-DD, where YYYY-MM-DD corresponds to the date of the snapshot.</p>
<h2 id="setup-for-running-a-databricks-spark-notebook-on-azure">Setup for running a databricks spark notebook on Azure</h2>
<blockquote>
<p>You will be able to run free queries for a time, but eventually you will have to pay for your compute time.</p>
</blockquote>
<p><strong>Create a Microsoft Azure account</strong> <a href="https://azure.microsoft.com/en-us/free/">here</a></p>
<p>I find it easiest to use the <a href="https://docs.microsoft.com/en-us/cli/azure/">az command line interface</a>. You can also set up using the Azure web interface. This tutorial is based off of the Databricks <a href="https://docs.microsoft.com/en-us/azure/databricks/scenarios/quickstart-create-databricks-workspace-portal?tabs=azure-portal">quickstart guide</a>.</p>
<p><strong>Download az</strong> <a href="https://docs.microsoft.com/en-us/cli/azure/">here</a>.</p>
<p><strong>Log into your account</strong> and <strong>install databricks</strong>.</p>
<div class="highlight"><pre tabindex="0" style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">az login
az extension add --name databricks
</code></pre></div><p>Create a <strong>resource group</strong> <code>gbif-resource-group</code>. The GBIF snapshot is located in <code>westeurope</code>.</p>
<div class="highlight"><pre tabindex="0" style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">az group create --name gbif-resource-group --location westeurope
az account list-locations -o table
</code></pre></div><p>Create a <strong>workspace</strong><code>gbif-ws</code>.</p>
<div class="highlight"><pre tabindex="0" style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">az databricks workspace create --resource-group gbif-resource-group --name gbif-ws --location westeurope --sku standard
</code></pre></div><p>From this page <a href="https://portal.azure.com/#blade/HubsExtension/BrowseResourceGroups">Azure web portal</a>, click on <strong>gbif-resource-group</strong>.</p>
<p><img src="/post/2021-04-22-microsoft-azure-and-gbif_files/resource_groups_screenshot.png" alt="launch workspace"></p>
<p>Click on <strong>launch workspace</strong>.</p>
<p><img src="/post/2021-04-22-microsoft-azure-and-gbif_files/launch_workspace.png" alt="launch workspace"></p>
<p>Click on <strong>new cluster</strong>.</p>
<p><img src="https://docs.microsoft.com/en-us/azure/databricks/scenarios/media/quickstart-create-databricks-workspace-portal/databricks-on-azure.png" alt="screenshot"></p>
<p>Create a <strong>new cluster</strong> named <code>mysparkcluster</code>. You can keep all of the default settings.</p>
<p><img src="https://docs.microsoft.com/en-us/azure/databricks/scenarios/media/quickstart-create-databricks-workspace-portal/create-databricks-spark-cluster.png" alt="screenshot"></p>
<p>Click <strong>create blank notebook</strong>.</p>
<p><img src="/post/2021-04-22-microsoft-azure-and-gbif_files/blank_notebook.png" alt="launch workspace"></p>
<p>Select the <strong>default language you want to use</strong> (R, scala, python).</p>
<p><img src="/post/2021-04-22-microsoft-azure-and-gbif_files/create_notebook.png" alt="launch workspace"></p>
<p>You should now be able to run the code below in <strong>your new notebook</strong>.</p>
<p><img src="/post/2021-04-22-microsoft-azure-and-gbif_files/scala_notebook.png" alt="launch workspace"></p>
<p>This code chunk will <strong>count the number of species (specieskeys) with occurrences in each country</strong>.</p>
<div class="highlight"><pre tabindex="0" style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-scala" data-lang="scala"><span style="color:#719e07">import</span> org.apache.spark.sql.functions._

<span style="color:#719e07">val</span> gbif_snapshot_path <span style="color:#719e07">=</span> <span style="color:#2aa198">&#34;wasbs://gbif@ai4edataeuwest.blob.core.windows.net/occurrence/2021-04-13/occurrence.parquet/*&#34;</span>

<span style="color:#719e07">val</span> df <span style="color:#719e07">=</span> spark<span style="color:#719e07">.</span>read<span style="color:#719e07">.</span>parquet<span style="color:#719e07">(</span>gbif_snapshot_path<span style="color:#719e07">)</span>
df<span style="color:#719e07">.</span>printSchema <span style="color:#586e75">// show columns
</span><span style="color:#586e75"></span>df<span style="color:#719e07">.</span>count<span style="color:#719e07">()</span> <span style="color:#586e75">// count the number of occurrences. 1.2B
</span><span style="color:#586e75"></span>
<span style="color:#586e75">// find the country with the most species
</span><span style="color:#586e75"></span><span style="color:#719e07">val</span> export_df <span style="color:#719e07">=</span> df<span style="color:#719e07">.</span>
select<span style="color:#719e07">(</span><span style="color:#2aa198">&#34;countrycode&#34;</span><span style="color:#719e07">,</span><span style="color:#2aa198">&#34;specieskey&#34;</span><span style="color:#719e07">).</span>
distinct<span style="color:#719e07">().</span>
groupBy<span style="color:#719e07">(</span><span style="color:#2aa198">&#34;countrycode&#34;</span><span style="color:#719e07">).</span>
count<span style="color:#719e07">().</span>
orderBy<span style="color:#719e07">(</span>desc<span style="color:#719e07">(</span><span style="color:#2aa198">&#34;count&#34;</span><span style="color:#719e07">))</span>

export_df<span style="color:#719e07">.</span>show<span style="color:#719e07">()</span>
</code></pre></div><p>The resulting <code>export_df</code> should look like this.</p>
<pre tabindex="0"><code>+-----------+------+
|countrycode| count|
+-----------+------+
|         US|187515|
|         AU|122746|
|         MX| 86399|
|         BR| 69167|
|         CA| 64422|
|         ZA| 60682|
</code></pre><p>Here is how you would count species by country using <strong>SparkR</strong>.</p>
<div class="highlight"><pre tabindex="0" style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r"><span style="color:#268bd2">library</span>(magrittr) <span style="color:#586e75"># for %&gt;% </span>
<span style="color:#268bd2">install.packages</span>(<span style="color:#2aa198">&#34;/databricks/spark/R/pkg&#34;</span>, repos <span style="color:#719e07">=</span> <span style="color:#cb4b16">NULL</span>)
<span style="color:#268bd2">library</span>(SparkR)

sc <span style="color:#719e07">=</span> <span style="color:#268bd2">sparkR.init</span>()
sqlContext <span style="color:#719e07">=</span> <span style="color:#268bd2">sparkRSQL.init</span>(sc)

gbif_snapshot_path <span style="color:#719e07">=</span> <span style="color:#2aa198">&#34;wasbs://gbif@ai4edataeuwest.blob.core.windows.net/occurrence/2021-04-13/occurrence.parquet/*&#34;</span>

df <span style="color:#719e07">=</span> <span style="color:#268bd2">read.parquet</span>(sqlContext,path<span style="color:#719e07">=</span>gbif_snapshot_path)
<span style="color:#268bd2">printSchema</span>(df)

export_df <span style="color:#719e07">=</span> df <span style="color:#719e07">%&gt;%</span> 
<span style="color:#268bd2">select</span>(<span style="color:#2aa198">&#34;countrycode&#34;</span>,<span style="color:#2aa198">&#34;specieskey&#34;</span>) <span style="color:#719e07">%&gt;%</span> 
<span style="color:#268bd2">distinct</span>() <span style="color:#719e07">%&gt;%</span>
<span style="color:#268bd2">groupBy</span>(<span style="color:#2aa198">&#34;countrycode&#34;</span>) <span style="color:#719e07">%&gt;%</span> 
<span style="color:#268bd2">count</span>() <span style="color:#719e07">%&gt;%</span>
<span style="color:#268bd2">arrange</span>(<span style="color:#2aa198">&#34;count&#34;</span>,decreasing <span style="color:#719e07">=</span> <span style="color:#cb4b16">TRUE</span>) 

export_df <span style="color:#719e07">%&gt;%</span> <span style="color:#268bd2">showDF</span>()
</code></pre></div><p>How you would count species by country in <strong>PySpark</strong>.</p>
<div class="highlight"><pre tabindex="0" style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#719e07">from</span> pyspark.sql <span style="color:#719e07">import</span> SQLContext
<span style="color:#719e07">from</span> pyspark.sql.functions <span style="color:#719e07">import</span> <span style="color:#719e07">*</span>
sqlContext <span style="color:#719e07">=</span> SQLContext(sc)

gbif_snapshot_path <span style="color:#719e07">=</span> <span style="color:#2aa198">&#34;wasbs://gbif@ai4edataeuwest.blob.core.windows.net/occurrence/2021-04-13/occurrence.parquet/*&#34;</span>

<span style="color:#586e75"># to read parquet file</span>
df <span style="color:#719e07">=</span> sqlContext<span style="color:#719e07">.</span>read<span style="color:#719e07">.</span>parquet(gbif_snapshot_path)
df<span style="color:#719e07">.</span>printSchema <span style="color:#586e75"># show columns</span>

<span style="color:#586e75"># find the country with the most species</span>
export_df <span style="color:#719e07">=</span> df\
<span style="color:#719e07">.</span>select(<span style="color:#2aa198">&#34;countrycode&#34;</span>,<span style="color:#2aa198">&#34;specieskey&#34;</span>)\
<span style="color:#719e07">.</span>distinct()\
<span style="color:#719e07">.</span>groupBy(<span style="color:#2aa198">&#34;countrycode&#34;</span>)\
<span style="color:#719e07">.</span>count()\
<span style="color:#719e07">.</span>orderBy(col(<span style="color:#2aa198">&#34;count&#34;</span>)<span style="color:#719e07">.</span>desc())

export_df<span style="color:#719e07">.</span>show()
</code></pre></div><h2 id="exporting-a-csv-table">Exporting a csv table</h2>
<p>We would now like to export <code>export_df</code> from the previous section into a csv file, so we can analyze it on our own computer. <strong>There is also a bit of setup involved here</strong>.</p>
<p>It is easiest to use the command line tool <code>az</code> ( <a href="https://docs.microsoft.com/en-us/cli/azure/">download here</a> ) to set up <strong>storage accounts</strong> and <strong>containers</strong> for storing your exported csv.</p>
<p>Replace <code>jwaller@gbif.org</code> with your Microsoft-Azure account name.</p>
<div class="highlight"><pre tabindex="0" style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">az login
az storage account create -n gbifblobstorage -g gbif-resource-group -l westeurope --sku Standard_LRS

az role assignment create --role <span style="color:#2aa198">&#34;Owner&#34;</span> --assignee <span style="color:#2aa198">&#34;jwaller@gbif.org&#34;</span>
az role assignment create --role <span style="color:#2aa198">&#34;Storage Blob Data Contributor&#34;</span> --assignee <span style="color:#2aa198">&#34;jwaller@gbif.org&#34;</span>
az role assignment create --role <span style="color:#2aa198">&#34;Storage Queue Data Contributor&#34;</span> --assignee <span style="color:#2aa198">&#34;jwaller@gbif.org&#34;</span>

az storage container create -n container1 --account-name gbifblobstorage --auth-mode login
</code></pre></div><p>Run this command to get the <strong>secret key</strong> (<code>sas_key</code>) you will need in the next section.</p>
<div class="highlight"><pre tabindex="0" style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">az storage account keys list -g gbif-resource-group -n gbifblobstorage
</code></pre></div><p>The <strong>secret key</strong> (<code>sas_key</code> in next section) you want is under <code>value</code>.</p>
<pre tabindex="0"><code>{
&quot;keyName&quot;: &quot;key1&quot;,
&quot;permissions&quot;: &quot;FULL&quot;,
&quot;value&quot;: &quot;copy_me_big_long_secret_key_kfaldkfalskdfj203932049230492f_fakekey_j030303fjdasfndsafldkj==&quot;
}
</code></pre><p>Now that the set up is over, you can <strong>export a dataframe</strong> as a csv file. The <strong>save path</strong> has the following form:</p>
<pre tabindex="0"><code>wasbs://container_name@storage_name.blob.core.windows.net/file_name.csv
</code></pre><p>We will use <strong>container1</strong> and <strong>gbifblobstorage</strong> created earlier. Copy your <code>sas_key</code> to replace my <strong>fake one</strong>. Run this in one of the notebooks you set up earlier.</p>
<div class="highlight"><pre tabindex="0" style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-scala" data-lang="scala"><span style="color:#719e07">val</span> sas_key <span style="color:#719e07">=</span> <span style="color:#2aa198">&#34;copy_me_big_long_secret_key_kfaldkfalskdfj203932049230492f_fakekey_j030303fjdasfndsafldkj==&#34;</span> <span style="color:#586e75">// fill the secret key from the previous section 
</span><span style="color:#586e75"></span>spark<span style="color:#719e07">.</span>conf<span style="color:#719e07">.</span>set<span style="color:#719e07">(</span><span style="color:#2aa198">&#34;fs.azure.account.key.gbifblobstorage.blob.core.windows.net&#34;</span><span style="color:#719e07">,</span>sas_key<span style="color:#719e07">)</span>

<span style="color:#586e75">// or you could export a parquet file
</span><span style="color:#586e75">// export_df.write.parquet(&#34;wasbs://container1@gbifblobstorage.blob.core.windows.net/export_df.parquet&#34;)
</span><span style="color:#586e75"></span>
export_df<span style="color:#719e07">.</span>
coalesce<span style="color:#719e07">(</span><span style="color:#2aa198">1</span><span style="color:#719e07">).</span>
write<span style="color:#719e07">.</span>
mode<span style="color:#719e07">(</span><span style="color:#2aa198">&#34;overwrite&#34;</span><span style="color:#719e07">).</span>
option<span style="color:#719e07">(</span><span style="color:#2aa198">&#34;header&#34;</span><span style="color:#719e07">,</span> <span style="color:#2aa198">&#34;true&#34;</span><span style="color:#719e07">).</span>
option<span style="color:#719e07">(</span><span style="color:#2aa198">&#34;sep&#34;</span><span style="color:#719e07">,</span> <span style="color:#2aa198">&#34;\t&#34;</span><span style="color:#719e07">).</span>
format<span style="color:#719e07">(</span><span style="color:#2aa198">&#34;csv&#34;</span><span style="color:#719e07">).</span>
save<span style="color:#719e07">(</span><span style="color:#2aa198">&#34;wasbs://container1@gbifblobstorage.blob.core.windows.net/export_df.csv&#34;</span><span style="color:#719e07">)</span>

</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r"><span style="color:#268bd2">library</span>(SparkR)
<span style="color:#268bd2">sparkR.session</span>()

sas_key <span style="color:#719e07">=</span> <span style="color:#2aa198">&#34;copy_me_big_long_secret_key_kfaldkfalskdfj203932049230492f_fakekey_j030303fjdasfndsafldkj==&#34;</span>
<span style="color:#268bd2">sparkR.session</span>(sparkConfig <span style="color:#719e07">=</span> <span style="color:#268bd2">list</span>(fs.azure.account.key.gbifblobstorage.blob.core.windows.net <span style="color:#719e07">=</span> sas_key))

export_df <span style="color:#719e07">%&gt;%</span>
<span style="color:#268bd2">repartition</span>(<span style="color:#2aa198">1</span>) <span style="color:#719e07">%&gt;%</span>
<span style="color:#268bd2">write.df</span>(<span style="color:#2aa198">&#34;wasbs://container1@gbifblobstorage.blob.core.windows.net/export_df.csv&#34;</span>, <span style="color:#2aa198">&#34;csv&#34;</span>, <span style="color:#2aa198">&#34;overwrite&#34;</span>)
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">sas_key <span style="color:#719e07">=</span> <span style="color:#2aa198">&#34;copy_me_big_long_secret_key_kfaldkfalskdfj203932049230492f_fakekey_j030303fjdasfndsafldkj==&#34;</span> <span style="color:#586e75"># fill the secret key from the previous section </span>
spark<span style="color:#719e07">.</span>conf<span style="color:#719e07">.</span>set(<span style="color:#2aa198">&#34;fs.azure.account.key.gbifblobstorage.blob.core.windows.net&#34;</span>,sas_key)

export_df\
<span style="color:#719e07">.</span>coalesce(<span style="color:#2aa198">1</span>)\
<span style="color:#719e07">.</span>write\
<span style="color:#719e07">.</span>mode(<span style="color:#2aa198">&#34;overwrite&#34;</span>)\
<span style="color:#719e07">.</span>option(<span style="color:#2aa198">&#34;header&#34;</span>, <span style="color:#2aa198">&#34;true&#34;</span>)\
<span style="color:#719e07">.</span>option(<span style="color:#2aa198">&#34;sep&#34;</span>, <span style="color:#2aa198">&#34;</span><span style="color:#cb4b16">\t</span><span style="color:#2aa198">&#34;</span>)\
<span style="color:#719e07">.</span>format(<span style="color:#2aa198">&#34;csv&#34;</span>)\
<span style="color:#719e07">.</span>save(<span style="color:#2aa198">&#34;wasbs://container1@gbifblobstorage.blob.core.windows.net/export_df.csv&#34;</span>)
</code></pre></div><p>You can now download the <code>export_df.csv</code> from the azure <strong>web interface</strong>. The file will not be a single csv file but a directory named <strong>export_df</strong>. Within this directory you will find several files. The one you are interested in will be something looking like this:</p>
<pre tabindex="0"><code>export_df.csv/part-00000-tid-54059299456474431-7c74a0a3-2332-423c-89ed-2d1a98427a01-449-1-c000.csv
</code></pre><h2 id="downloading-the-csv">Downloading the csv</h2>
<p>Go to your <a href="https://portal.azure.com/#blade/HubsExtension/BrowseResource/resourceType/Microsoft.Storage%2FStorageAccounts">storage account</a>.</p>
<p>Click on <strong>gbifblobstorage</strong>.</p>
<p><img src="/post/2021-04-22-microsoft-azure-and-gbif_files/gbifblobstorage.png" alt="gbifblobstorage"></p>
<p>Click on <strong>containters</strong>.</p>
<p><img src="/post/2021-04-22-microsoft-azure-and-gbif_files/containers.png" alt="gbifblobstorage"></p>
<p>Download the <strong>csv</strong>.</p>
<p><code>export_df.csv/part-00000-tid-54059299456474431-7c74a0a3-2332-423c-89ed-2d1a98427a01-449-1-c000.csv</code>.</p>
<p><img src="/post/2021-04-22-microsoft-azure-and-gbif_files/part.png" alt="gbifblobstorage"></p>
<h2 id="citing-custom-filteredprocessed-data">Citing custom filtered/processed data</h2>
<p>If you end up using your <strong>exported csv</strong> file in a research paper, you will want a <strong>doi</strong>. GBIF now has a <a href="https://www.gbif.org/derived-dataset/register">service</a> for generating a <strong>citable doi</strong> from <strong>a list of involved datasetkeys with occurrences counts</strong>. See the <a href="https://www.gbif.org/citation-guidelines">GBIF citation guidelines</a> and <a href="https://data-blog.gbif.org/post/derived-datasets/">previous blog post</a>.</p>
<p>You can generate a <strong>citation file</strong> for your custom dataset above using the following code chunk. Since our <code>export_df.csv</code> used all of the occurrences, we can simply group by datasetkey and count all of the occurrences to generate our <code>citation.csv</code> file.</p>
<div class="highlight"><pre tabindex="0" style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-scala" data-lang="scala"><span style="color:#719e07">val</span> sas_key <span style="color:#719e07">=</span> <span style="color:#2aa198">&#34;copy_me_big_long_secret_key_kfaldkfalskdfj203932049230492f_fakekey_j030303fjdasfndsafldkj==&#34;</span> <span style="color:#586e75">// fill the secret key from the previous section 
</span><span style="color:#586e75"></span>spark<span style="color:#719e07">.</span>conf<span style="color:#719e07">.</span>set<span style="color:#719e07">(</span><span style="color:#2aa198">&#34;fs.azure.account.key.gbifblobstorage.blob.core.windows.net&#34;</span><span style="color:#719e07">,</span>sas_key<span style="color:#719e07">)</span>

<span style="color:#719e07">import</span> org.apache.spark.sql.functions._

<span style="color:#719e07">val</span> gbif_snapshot_path <span style="color:#719e07">=</span> <span style="color:#2aa198">&#34;wasbs://gbif@ai4edataeuwest.blob.core.windows.net/occurrence/2021-04-13/occurrence.parquet/*&#34;</span>

<span style="color:#719e07">val</span> citation_df <span style="color:#719e07">=</span> spark<span style="color:#719e07">.</span>read<span style="color:#719e07">.</span>parquet<span style="color:#719e07">(</span>gbif_snapshot_path<span style="color:#719e07">).</span>
groupBy<span style="color:#719e07">(</span><span style="color:#2aa198">&#34;datasetkey&#34;</span><span style="color:#719e07">).</span>
count<span style="color:#719e07">()</span>

citation_df<span style="color:#719e07">.</span>
coalesce<span style="color:#719e07">(</span><span style="color:#2aa198">1</span><span style="color:#719e07">).</span>
write<span style="color:#719e07">.</span>
mode<span style="color:#719e07">(</span><span style="color:#2aa198">&#34;overwrite&#34;</span><span style="color:#719e07">).</span>
option<span style="color:#719e07">(</span><span style="color:#2aa198">&#34;header&#34;</span><span style="color:#719e07">,</span> <span style="color:#2aa198">&#34;true&#34;</span><span style="color:#719e07">).</span>
option<span style="color:#719e07">(</span><span style="color:#2aa198">&#34;sep&#34;</span><span style="color:#719e07">,</span> <span style="color:#2aa198">&#34;\t&#34;</span><span style="color:#719e07">).</span>
format<span style="color:#719e07">(</span><span style="color:#2aa198">&#34;csv&#34;</span><span style="color:#719e07">).</span>
save<span style="color:#719e07">(</span><span style="color:#2aa198">&#34;wasbs://container1@gbifblobstorage.blob.core.windows.net/citation.csv&#34;</span><span style="color:#719e07">)</span>
</code></pre></div><p>You can also now use your <strong>citation file</strong> with the <strong>development version</strong> of <code>rgbif</code> to create a derived dataset and a citable DOI, <strong>although you would need to upload your exported dataset to Zenodo (or something similar) first</strong>.</p>
<div class="highlight"><pre tabindex="0" style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r"><span style="color:#586e75"># pak::pkg_install(&#34;ropensci/rgbif&#34;) # requires development version of rgbif</span>

<span style="color:#268bd2">library</span>(rgbif)

citation_data <span style="color:#719e07">=</span> readr<span style="color:#719e07">::</span><span style="color:#268bd2">read_tsv</span>(<span style="color:#2aa198">&#34;citation.csv&#34;</span>)

<span style="color:#586e75"># use derived_dataset_prep() if you just want to test</span>
<span style="color:#268bd2">derived_dataset</span>(
citation_data <span style="color:#719e07">=</span> citation_data,
title <span style="color:#719e07">=</span> <span style="color:#2aa198">&#34;Research dataset derived from GBIF snapshot on MS Azure.&#34;</span>,
description <span style="color:#719e07">=</span> <span style="color:#2aa198">&#34;I used Azure and apache-spark to filter GBIF snapshot 2021-04-13.&#34;</span>,
source_url <span style="color:#719e07">=</span> <span style="color:#2aa198">&#34;https://zenodo.org/fake_upload&#34;</span>,
user<span style="color:#719e07">=</span><span style="color:#2aa198">&#34;your_gbif_user_name&#34;</span>,
pwd<span style="color:#719e07">=</span><span style="color:#2aa198">&#34;your_gbif_password&#34;</span>
)
</code></pre></div><p>The derived dataset with a <strong>citable DOI</strong> will appear on your <a href="https://www.gbif.org/derived-dataset">gbif user page</a>.</p>
<p>Hopefully you have everything that you need to start using GBIF mediated occurrence data on Microsoft Azure. Please leave a comment if something does not work for you.</p>
    </div>

    
    

    
    

    <footer class="post-footer">
      <div class="post-tags">
          
          <a href="/tags/cloud-computing/">cloud computing</a>
          
          <a href="/tags/azure/">azure</a>
          
        </div>

      
      <nav class="post-nav">
        
          <a class="prev" href="/post/derived-datasets/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">Derived datasets</span>
            <span class="prev-text nav-mobile">Prev</span>
          </a>
        
          <a class="next" href="/post/grscicoll-2021/">
            <span class="next-text nav-default">The Global Registry of Scientific Collections (GRSciColl) in 2021</span>
            <span class="next-text nav-mobile">Next</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        

      </div>
    </main>

    <footer id="footer" class="footer">
      



<div>
<h3>This post represents the ideas of the author.</h3>
</div>
<div id='discourse-comments'></div>
<script type="text/javascript">
  DiscourseEmbed = { discourseUrl: 'https://discourse.gbif.org/',
                     discourseEmbedUrl: 'https://data-blog.gbif.org\/post\/microsoft-azure-and-gbif\/' };

  (function() {
    var d = document.createElement('script'); d.type = 'text/javascript'; d.async = true;
    d.src = DiscourseEmbed.discourseUrl + 'javascripts/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(d);
  })();
</script>


<div class = "copyright">
	<a class="theme-link" href="https://www.gbif.org/what-is-gbif">What is GBIF?</a>
	<span class="division">|</span>
	<a class="theme-link" href="https://www.gbif.org/developer/summary">API</a>
	<span class="division">|</span>
	<a class="theme-link" href="https://www.gbif.org/faq">FAQ</a>
	<span class="division">|</span>
	<a class="theme-link" href="https://discourse.gbif.org/">Community-Forum</a>
</div>

<div class = "copyright">
	<span class="theme-info">GBIF Secretariat</span>
	<span class="theme-info">Universitetsparken 15</span>
	<span class="theme-info">DK-2100 Copenhagen Ø</span>
	<span class="theme-info" >Denmark</span>
</div>

<div class="copyright">
	<span class="power-by">
	Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
	</span>
	<span class="division">|</span>
	<span class="theme-info">
	Theme -
	<a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
	</span>

</div>

<script defer data-domain="data-blog.gbif.org,gbif.org" src="https://plausible.io/js/script.file-downloads.js"></script>

    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
<script src="/lib/highlight/highlight.pack.js?v=20171001"></script><script type="text/javascript" src="/lib/jquery/jquery-3.2.1.min.js"></script>
  <script type="text/javascript" src="/lib/slideout/slideout-1.0.1.min.js"></script>
  


<script type="text/javascript" src="/dist/even.min.js?v=3.2.0"></script>








</body>
</html>
